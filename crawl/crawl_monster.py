# ‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø
# ‚†ø‚†ø‚†ø‚†ø‚†ø‚¢ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚†ø‚†ø‚†ü‚†õ‚†õ‚†õ‚†õ‚†õ
# ‚£∂‚£¶‚£§‚£§‚£§‚£§‚£§‚£§‚£¨‚£≠‚£≠‚£ç‚£â‚°ô‚†õ‚†ª‚¢ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚°ø‚†õ‚£ã‚£©‚£≠‚£•‚£§‚£¥‚£∂‚£∂‚£∂‚£∂‚£∂‚£∂‚£∂‚£∂‚£∂
# ‚£Ü‚†Ä‚†Ä‚†Ä‚¢°‚†Å‚†Ä‚°Ä‚†Ä‚¢∏‚†ü‚†ª‚£Ø‚†ô‚†õ‚†∑‚£∂‚£¨‚°ô‚†ª‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚†ü‚¢â‚£•‚£∂‚°ü‚†ª‚£ô‚°â‚†Ä‚¢∞‚°Ü‚†Ä‚†Ä‚£°‚†Ä‚£ß‚†Ä‚†Ä‚†Ä‚¢®
# ‚†ª‚£¶‚†Ä‚†Ä‚†à‚£á‚£Ä‚£ß‚£¥‚£ø‚£∂‚£∂‚£ø‚£∑‚†Ä‚¢Ä‚°á‚†â‚†ª‚¢∂‚£å‚¢ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£°‚°∂‚†ü‚†â‚†Ä‚¢£‚†Ä‚£ø‚†∑‚†Ä‚†Ä‚†Ä‚†Ä‚£ø‚°∑‚¢Ä‚†á‚†Ä‚†Ä‚¢†‚£ø
# ‚£¶‚°à‚¢ß‚°Ä‚†Ä‚†ò‚¢Æ‚°ô‚†õ‚†â‚†Ä‚†Ñ‚†ô‚¢ø‚£Ä‚†û‚†Ä‚†Ä‚†Ä‚†Ä‚†ô‚¢ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚°ø‚†ã‚†Ä‚†Ä‚†Ä‚†Ä‚†à‚†≥‚£Ñ‚†â‚†ì‚†í‚†ö‚†ã‚¢Ä‚°†‚†ã‚†Ä‚¢Ä‚£¥‚£è‚£ø
# ‚£ø‚£ø‚£ø‚£õ‚£¶‚£Ä‚†Ä‚†ô‚†ì‚†¶‚†§‚£§‚†î‚†õ‚†Å‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢Ä‚£Ä‚£π‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£§‚£§‚£§‚£§‚£§‚£Ä‚£Ä‚£Ä‚£Ä‚¢ô‚¢ì‚£í‚°í‚†ö‚†ã‚¢†‚£§‚¢∂‚£ü‚£Ω‚£ø‚£ø
# ‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£∑‚£¶‚†Ä‚†Ä‚£¥‚£ø‚£∑‚£∂‚£∂‚£∂‚£æ‚°ñ‚¢∞‚£æ‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£∂‚£∂‚£æ‚£ø‚£ø‚£∂‚£æ‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø
# ‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚†Ä‚¢Ä‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚†É‚£∏‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø
# ‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚°è‚†Ä‚¢∏‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚†Å‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø
# ‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£∑‚†Ä‚¢∏‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚†Ä‚£ª‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø
     
#                           üîí Licensed under the GNU GPLv3
#                    üåê https://www.gnu.org/licenses/agpl-3.0.html
# ¬†¬†¬†¬†¬†¬†¬†¬†                   https://t.me/amorescam

import re
import urllib.request
import numpy as np
from urllib.parse import urlparse
from bs4 import BeautifulSoup, SoupStrainer

# helper to parse the url


def parse_url(url):
    # parse out the page name and base_url
    last_slash_idx = url.rfind('/')
    doc_name = url[last_slash_idx + 1:]
    base_url = url[:last_slash_idx + 1]
    return base_url, doc_name


class PageRankIndex(object):
    def __init__(self):
        # you'll want to create something here to hold your index, and other
        # necessary data members
        # list to hold doc urls (enable getting indices of the docs)
        self.doc_urls = []
        self.documents = {}  # map to hold doc id and html page
        self.inverted_index = {}  # map for inverted index: term - posting lists

    # index_url( url )
    # purpose: crawl through a web directory of html files and generate an
    #   index of the contents
    # preconditions: none
    # returns: num of documents indexed
    # hint: use BeautifulSoup and urllib
    # parameters:
    #   url - a string containing a url to begin indexing at

    def index_url(self, url):
        base_url, doc_name = parse_url(url)
        # read the web page
        with urllib.request.urlopen(url) as read_url:
            html_page = read_url.read()

        soup = BeautifulSoup(html_page, "html.parser")
        divTag = soup.find_all("div", {"class": "js_result_details"})
        for tag in divTag:
            jobTitleTag = tag.find("div", {"class": "jobTitle"})

            h2Tags = jobTitleTag.find("h2")
            aTags = h2Tags.find("a")

            for tag in aTags:
                l = tag.get("href")
                if l[:4] != 'http':
                    l = base_url + l

                # add neighbors to the node in the graph
                self.doc_urls.append(l)
                doc_idx = self.doc_urls.index(l)

                # index the web page content
                # self.doc_urls.append(url)
                doc_idx = self.doc_urls.index(l)
                self.build_inverted_index(doc_idx, soup.get_text())

        return len(self.doc_urls)

    # build inverted index
    def build_inverted_index(self, doc_id, doc_text):
        tokens = self.tokenize(doc_text)
        for t in tokens:
            if t not in self.inverted_index:
                self.inverted_index[t] = set()
            self.inverted_index[t].add(doc_id)

    # tokenize( text )
    # purpose: convert a string of terms into a list of terms
    # preconditions: none
    # returns: list of terms contained within the text
    # parameters:
    #   text - a string of terms
    def tokenize(self, text):
        # ADD CODE HERE
        clean_string = re.sub('[^a-z0-9 ]', ' ', text.lower())
        tokens = clean_string.split()
        return tokens

    # ranked_search( text )
    # purpose: searches for the terms in "text" in our index and returns
    #   AND results for highest 10 ranked results
    # preconditions: .index_url(...) has been called on our corpus
    # returns: list of tuples of (url,PageRank) containing relevant
    #   search results
    # parameters:
    #   text - a string of query terms

    def ranked_search(self, text):
        results_set = set(range(0, len(self.doc_urls)))
        search_terms = self.tokenize(text)

        for term in search_terms:
            # if one of the terms not in the doc - no result
            if term not in self.inverted_index:
                results_set = set()
            else:
                results_set = results_set.intersection(
                    set(self.inverted_index.get(term)))

        return results_set


# now, we'll define our main function which actually starts the indexer and
# does a few queries
def main(args):
    index = PageRankIndex()
    url = "https://www.monster.com/jobs/search/?q=software-engineer&intcid=skr_navigation_nhpso_searchMain"

    num_files = index.index_url(url)

    # added code
    # index.calculate_transition_matrix()
    # index.matrix_converge()
    # End of my added code
    # print(index.doc_urls)
    # print(index.inverted_index)
    search_queries = (
        'java', 'python ', 'software development', 'big data', 'machine learning'
    )
    for q in search_queries:
        results = index.ranked_search(q)
        print("searching: %s -- results: %s" % (q, results))


# this little helper will call main() if this file is executed from the command
# line but not call main() if this file is included as a module
if __name__ == "__main__":
    import sys
    main(sys.argv)
