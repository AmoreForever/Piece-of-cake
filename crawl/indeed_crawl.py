# ‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø
# ‚†ø‚†ø‚†ø‚†ø‚†ø‚¢ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚†ø‚†ø‚†ü‚†õ‚†õ‚†õ‚†õ‚†õ
# ‚£∂‚£¶‚£§‚£§‚£§‚£§‚£§‚£§‚£¨‚£≠‚£≠‚£ç‚£â‚°ô‚†õ‚†ª‚¢ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚°ø‚†õ‚£ã‚£©‚£≠‚£•‚£§‚£¥‚£∂‚£∂‚£∂‚£∂‚£∂‚£∂‚£∂‚£∂‚£∂
# ‚£Ü‚†Ä‚†Ä‚†Ä‚¢°‚†Å‚†Ä‚°Ä‚†Ä‚¢∏‚†ü‚†ª‚£Ø‚†ô‚†õ‚†∑‚£∂‚£¨‚°ô‚†ª‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚†ü‚¢â‚£•‚£∂‚°ü‚†ª‚£ô‚°â‚†Ä‚¢∞‚°Ü‚†Ä‚†Ä‚£°‚†Ä‚£ß‚†Ä‚†Ä‚†Ä‚¢®
# ‚†ª‚£¶‚†Ä‚†Ä‚†à‚£á‚£Ä‚£ß‚£¥‚£ø‚£∂‚£∂‚£ø‚£∑‚†Ä‚¢Ä‚°á‚†â‚†ª‚¢∂‚£å‚¢ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£°‚°∂‚†ü‚†â‚†Ä‚¢£‚†Ä‚£ø‚†∑‚†Ä‚†Ä‚†Ä‚†Ä‚£ø‚°∑‚¢Ä‚†á‚†Ä‚†Ä‚¢†‚£ø
# ‚£¶‚°à‚¢ß‚°Ä‚†Ä‚†ò‚¢Æ‚°ô‚†õ‚†â‚†Ä‚†Ñ‚†ô‚¢ø‚£Ä‚†û‚†Ä‚†Ä‚†Ä‚†Ä‚†ô‚¢ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚°ø‚†ã‚†Ä‚†Ä‚†Ä‚†Ä‚†à‚†≥‚£Ñ‚†â‚†ì‚†í‚†ö‚†ã‚¢Ä‚°†‚†ã‚†Ä‚¢Ä‚£¥‚£è‚£ø
# ‚£ø‚£ø‚£ø‚£õ‚£¶‚£Ä‚†Ä‚†ô‚†ì‚†¶‚†§‚£§‚†î‚†õ‚†Å‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢Ä‚£Ä‚£π‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£§‚£§‚£§‚£§‚£§‚£Ä‚£Ä‚£Ä‚£Ä‚¢ô‚¢ì‚£í‚°í‚†ö‚†ã‚¢†‚£§‚¢∂‚£ü‚£Ω‚£ø‚£ø
# ‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£∑‚£¶‚†Ä‚†Ä‚£¥‚£ø‚£∑‚£∂‚£∂‚£∂‚£æ‚°ñ‚¢∞‚£æ‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£∂‚£∂‚£æ‚£ø‚£ø‚£∂‚£æ‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø
# ‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚†Ä‚¢Ä‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚†É‚£∏‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø
# ‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚°è‚†Ä‚¢∏‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚†Å‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø
# ‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£∑‚†Ä‚¢∏‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚†Ä‚£ª‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø
     
#                           üîí Licensed under the GNU GPLv3
#                    üåê https://www.gnu.org/licenses/agpl-3.0.html
# ¬†¬†¬†¬†¬†¬†¬†¬†                   https://t.me/amorescam

from lxml import html
import requests
import re
import os
import sys
import unicodecsv as csv
import argparse
import json


def indeed_crawl(keyword, city, state):
    print(f"Fetching {keyword} at {city},{state}")

    # convert place id
    place_id = f"{city}%2C+{state}"

    # Form GET request like this one
    # https://www.monster.com/jobs/search/?q=software-engineer&where=Boston__2C-MA&page=1
    job_litsting_url = 'https://www.indeed.com/jobs'
    params = {
        'q': keyword,
        'l': place_id
    }
    response = requests.get(job_litsting_url, params=params)

    # Extract data
    parser = html.fromstring(response.text)

    # Making absolute url
    base_url = "https://www.monster.com/"
    parser.make_links_absolute(base_url)

    XPATH_ALL_JOB = '//div[@class="summary"]'
    XPATH_NAME = './/a/text()'
    XPATH_JOB_URL = './/a/@href'
    XPATH_LOC = './/div[@class="location"]/span/text()'
    XPATH_COMPANY = './/div[@class="company"]/span/text()'

    # Loop to get each job posting
    job_listings = []
    listings = parser.xpath(XPATH_ALL_JOB)
    for job in listings:
        raw_job_name = job.xpath(XPATH_NAME)
        raw_job_url = job.xpath(XPATH_JOB_URL)
        raw_lob_loc = job.xpath(XPATH_LOC)
        raw_company = job.xpath(XPATH_COMPANY)

        # Clean name
        job_name = ''.join(raw_job_name).replace(
            '*', '').strip() if raw_job_name else None

        # Clean location
        job_location = ''.join(raw_lob_loc) if raw_lob_loc else None
        raw_state = re.findall(",\s?(.*)\s?", job_location)
        state = ''.join(raw_state).strip()
        raw_city = job_location.replace(state, '')
        city = raw_city.replace(',', '').strip()

        # Clean company name
        company = ''.join(raw_company).replace('‚Äì', '')

        # Clean job url
        job_url = raw_job_url[0] if raw_job_url else None

        jobs = {
            "Name": job_name,
            "Company": company,
            "City": city,
            "State": state,
            "Url": job_url
        }
        job_listings.append(jobs)

    return job_listings


if __name__ == "__main__":

    ''' eg-:python whatever.py "Android developer" "new york" "NY" '''

    argparser = argparse.ArgumentParser()
    argparser.add_argument('keyword', help='job name', type=str)
    argparser.add_argument('city', help='job location', type=str)
    argparser.add_argument('state', help='state location', type=str)
    args = argparser.parse_args()
    keyword = args.keyword
    city = args.city
    state = args.state
    # keyword = "software-engineer"
    # city = "Boston"
    # state = "MA"
    scraped_data = indeed_crawl(keyword, city, state)
    # print(scraped_data)

    print("Writing data to output file")
    with open(f'monster-{keyword}-{city}-{state}-job-results.csv', 'wb') as csvfile:
        fieldnames = ['Name', 'Company', 'City', 'State', 'Url']
        writer = csv.DictWriter(
            csvfile, fieldnames=fieldnames, quoting=csv.QUOTE_ALL)
        writer.writeheader()
        if scraped_data:
            for data in scraped_data:
                writer.writerow(data)
        else:
            print(f"Your search for {keyword}, in {city},{state} does not match any jobs")
