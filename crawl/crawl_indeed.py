import re
import urllib.request
import numpy as np
from urllib.parse import urlparse
from bs4 import BeautifulSoup, SoupStrainer

# helper to parse the url 
def parse_url(url):
    # parse out the page name and base_url
    last_slash_idx = url.rfind('/')
    doc_name = url[last_slash_idx + 1:]
    base_url = url[:last_slash_idx + 1]
    return base_url, doc_name 


class PageRankIndex(object):
    def __init__(self):
        # you'll want to create something here to hold your index, and other
        # necessary data members
        self.doc_urls = [] # list to hold doc urls (enable getting indices of the docs)  
        self.documents = {} # map to hold doc id and html page      
        self.inverted_index = {} # map for inverted index: term - posting lists 

    
    # index_url( url )
    # purpose: crawl through a web directory of html files and generate an
    #   index of the contents
    # preconditions: none
    # returns: num of documents indexed
    # hint: use BeautifulSoup and urllib
    # parameters:
    #   url - a string containing a url to begin indexing at
    def index_url(self, url):
        base_url, doc_name = parse_url(url)
        # read the web page
        with urllib.request.urlopen(url) as read_url:
            html_page = read_url.read()
        
        soup = BeautifulSoup(html_page, "html.parser")
        divTag = soup.find_all("div", {"class": "row result"})

        for tag in divTag:
            tdTags = tag.find_all("a", {"class": "jobtitle"})

            for tag in tdTags:
                print(tag.text)
                l = tag.get("href")
                if l[:4] != 'http':
                    l = base_url + l 

                # add neighbors to the node in the graph 
                self.doc_urls.append(l)
                doc_idx = self.doc_urls.index(l)

                # index the web page content
                # self.doc_urls.append(url)
                doc_idx = self.doc_urls.index(l)
                self.build_inverted_index(doc_idx, soup.get_text())
                
        return len(self.doc_urls)

    # build inverted index
    def build_inverted_index(self, doc_id, doc_text):
        tokens = self.tokenize(doc_text)
        for t in tokens:
            if t not in self.inverted_index:
                self.inverted_index[t] = set()
            self.inverted_index[t].add(doc_id)
        
    # tokenize( text )
    # purpose: convert a string of terms into a list of terms 
    # preconditions: none
    # returns: list of terms contained within the text
    # parameters:
    #   text - a string of terms
    def tokenize(self, text):
        # ADD CODE HERE
        clean_string = re.sub('[^a-z0-9 ]', ' ', text.lower())
        tokens = clean_string.split()
        return tokens

    
    # ranked_search( text )
    # purpose: searches for the terms in "text" in our index and returns
    #   AND results for highest 10 ranked results
    # preconditions: .index_url(...) has been called on our corpus
    # returns: list of tuples of (url,PageRank) containing relevant
    #   search results
    # parameters:
    #   text - a string of query terms
    def ranked_search(self, text):
        results_set = set(range(0, len(self.doc_urls)))
        search_terms = self.tokenize(text)

        for term in search_terms: 
            # if one of the terms not in the doc - no result
            if term not in self.inverted_index:
                results_set = set()
            else:
                results_set = results_set.intersection(set(self.inverted_index.get(term)))
        
        return results_set 


# now, we'll define our main function which actually starts the indexer and
# does a few queries
def main(args):
    index = PageRankIndex()
    url = "https://www.indeed.com/q-software-engineer-jobs.html"

    num_files = index.index_url(url)

    ### added code  
    # index.calculate_transition_matrix()
    # index.matrix_converge()
    ### End of my added code 
    # print(index.doc_urls)
    # print(index.inverted_index)
    search_queries = (
       'java', 'python ', 'software development', 'big data', 'machine learning'
        )
    for q in search_queries:
        results = index.ranked_search(q)
        print("searching: %s -- results: %s" % (q, results))


# this little helper will call main() if this file is executed from the command
# line but not call main() if this file is included as a module
if __name__ == "__main__":
    import sys
    main(sys.argv)